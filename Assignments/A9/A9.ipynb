{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9 - NLP using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get to work with recurrent network architectures with application to language processing tasks and observe behaviour of the learning using tensorboard visualization.\n",
    "\n",
    "You'll learn to use\n",
    "\n",
    " * word embeddings,\n",
    " * LSTMs,\n",
    " * tensorboard visualization to develop and tune deep learning architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the deep learning environment in the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same kind of preparation as in [Assignment 5](../A5/A5-instruction.html) we are going to use **[pytorch](http://pytorch.org)** for the deep learning aspects of the assignment.\n",
    "\n",
    "There is a pytorch setup in the big data under the globally available anaconda installation.\n",
    "However, it is recommended that you use the custom **dlenv** conda environment that contains all python package dependencies that are relevant for this assignment (and also nltk, gensim, tensorflow, keras, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either you load it directly\n",
    "```\n",
    "source activate /usr/shared/CMPT/big-data/tmp_py/dlenv\n",
    "```\n",
    "or you prepare\n",
    "```\n",
    "cd ~\n",
    "mkdir -p .conda/envs\n",
    "ln -s /usr/shared/CMPT/big-data/tmp_py/dlenv .conda/envs\n",
    "```\n",
    "and from thereon simply use\n",
    "```\n",
    "source activate dlenv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "bdenv_loc = '/usr/shared/CMPT/big-data'\n",
    "bdata = os.path.join(bdenv_loc,'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Explore Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are mappings between words and multi-dimensional vectors, where the difference between two word vectors has some relationship with the meaning of the corresponding words, i.e. words that are similar in meaning are mapped closely together (ideally). This part of the assignment should enable you to\n",
    "\n",
    "* Load a pretrained word embedding\n",
    "* Perform basic operations, such as distance queries and evaluate simple analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# Load Google's pre-trained Word2Vec model, trained on news articles\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    os.path.join(bdata,'GoogleNews-vectors-negative300.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read up about the word2vec API in gensim and\n",
    "# obtain a vector representation for a word of your choice\n",
    "\n",
    "...\n",
    "\n",
    "# to confirm that this worked, print out the number of elements of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the 10 words that are closest in the embedding to the word vector your produced above\n",
    "\n",
    "...\n",
    "\n",
    "# are the nearest neighbours similar in meaning?\n",
    "# try different seed words, until you find one whose neighbourhood looks OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a combination of positive and negative words, find out which word is most\n",
    "# similar to woman + king - man\n",
    "\n",
    "...\n",
    "\n",
    "# note, gensim's API allows you to combine positive and negative words without obtainng their vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may find that the results of most word analogy combinations don't work as well as we'd hope.\n",
    "# however, explore a bit and find two more cases where the output of your word vector algebra makes sense.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Sequence modeling with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will get to use a learning and a rule-based model of text sentiment analysis. To keep things simple, you will receive almost all the code and are just left with the task to tune the given algorithms, see the part about instrumentation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a simple LSTM model that is capable of producing a lable for a sequence of vector encoded words, based on code from [this repo](https://github.com/clairett/pytorch-sentiment-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size,\n",
    "                 use_gpu, batch_size, dropout=0.5, bidirectional=False):\n",
    "        \"\"\"Prepare individual layers\"\"\"\n",
    "        super(LSTMSentiment, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=bidirectional)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*self.num_directions, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"Choose appropriate size and type of hidden layer\"\"\"\n",
    "        # first is the hidden h\n",
    "        # second is the cell c\n",
    "        if self.use_gpu:\n",
    "            return (Variable(torch.zeros(self.num_directions, self.batch_size, self.hidden_dim).cuda()),\n",
    "                    Variable(torch.zeros(self.num_directions, self.batch_size, self.hidden_dim).cuda()))\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.num_directions, self.batch_size, self.hidden_dim)),\n",
    "                    Variable(torch.zeros(self.num_directions, self.batch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"Use the layers of this model to propagate input and return class log probabilities\"\"\"\n",
    "        if self.use_gpu:\n",
    "            sentence = sentence.cuda()\n",
    "        x = self.embeddings(sentence).view(len(sentence), self.batch_size, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        \n",
    "        y = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y, dim=0)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import time, random\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm.write = print\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        for line in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1).decode('latin-1')\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            if word in vocab:\n",
    "               word_vecs[word] = np.frombuffer(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return word_vecs\n",
    "\n",
    "\n",
    "def get_accuracy(truth, pred):\n",
    "    assert len(truth) == len(pred)\n",
    "    right = 0\n",
    "    for i in range(len(truth)):\n",
    "        if truth[i].item() == pred[i]:\n",
    "            right += 1.0\n",
    "    return right / len(truth)\n",
    "\n",
    "\n",
    "def train_epoch_progress(model, train_iter, loss_function, optimizer, text_field, label_field, epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    count = 0\n",
    "    for batch in tqdm(train_iter, desc='Train epoch '+str(epoch+1)):\n",
    "        sent, label = batch.text, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        if USE_GPU:\n",
    "            pred_label = pred.data.max(1)[1].cpu().numpy()\n",
    "        else:\n",
    "            pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data.item()\n",
    "        count += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_iter)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def train_epoch(model, train_iter, loss_function, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    count = 0\n",
    "    for batch in train_iter:\n",
    "        sent, label = batch.text, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        if USE_GPU:\n",
    "            pred_label = pred.data.max(1)[1].cpu().numpy()\n",
    "        else:\n",
    "            pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data.item()\n",
    "        count += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_iter)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, data, loss_function, name):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    for batch in data:\n",
    "        sent, label = batch.text, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        if USE_GPU:\n",
    "            pred_label = pred.data.max(1)[1].cpu().numpy()\n",
    "        else:\n",
    "            pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data.item()\n",
    "    avg_loss /= len(data)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    print(name + ': loss %.2f acc %.1f' % (avg_loss, acc*100))\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def load_sst(text_field, label_field, batch_size, use_gpu=True):\n",
    "    train, dev, test = data.TabularDataset.splits(path=os.path.join(bdata,'sst2'), train='train.tsv',\n",
    "                                                  validation='dev.tsv', test='test.tsv', format='tsv',\n",
    "                                                  fields=[('text', text_field), ('label', label_field)])\n",
    "    text_field.build_vocab(train, dev, test)\n",
    "    label_field.build_vocab(train, dev, test)\n",
    "    train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test),\n",
    "                                                                 batch_sizes=(batch_size, len(dev), len(test)),\n",
    "                                                                 sort_key=lambda x: len(x.text), repeat=False,\n",
    "                                                                 device=0 if use_gpu else -1)\n",
    "    return train_iter, dev_iter, test_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: after instrumention (see below) tune these parameters to improve the performance of the model\n",
    "EPOCHS = 5\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "#EMBEDDING_TYPE = 'glove'\n",
    "EMBEDDING_TYPE = 'word2vec'\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 10\n",
    "USE_BILSTM = False\n",
    "DROPOUT = .05\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "timestamp = str(int(time.time()))\n",
    "best_dev_acc = 0.0\n",
    "\n",
    "text_field = data.Field(lower=True)\n",
    "label_field = data.Field(sequential=False)\n",
    "train_iter, dev_iter, test_iter = load_sst(text_field, label_field, BATCH_SIZE, USE_GPU)\n",
    "\n",
    "model = LSTMSentiment(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                      vocab_size=len(text_field.vocab), label_size=len(label_field.vocab)-1,\\\n",
    "                      use_gpu=USE_GPU, batch_size=BATCH_SIZE, dropout=DROPOUT, bidirectional=USE_BILSTM)\n",
    "\n",
    "if USE_GPU:\n",
    "    model = model.cuda()\n",
    "\n",
    "best_model = model\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'glove' in EMBEDDING_TYPE:\n",
    "    #text_field.vocab.load_vectors('glove.6B.{}d'.format(EMBEDDING_DIM))\n",
    "    text_field.vocab.load_vectors('glove.twitter.27B.100d')\n",
    "    if USE_GPU:\n",
    "        model.embeddings.weight.data = text_field.vocab.vectors.cuda()\n",
    "    else:\n",
    "        model.embeddings.weight.data = text_field.vocab.vectors\n",
    "    #model.embeddings.embed.weight.requires_grad = False\n",
    "elif 'word2vec' in EMBEDDING_TYPE:\n",
    "    word_to_idx = text_field.vocab.stoi\n",
    "    pretrained_embeddings = np.random.uniform(-0.25, 0.25, (len(text_field.vocab), 300))\n",
    "    pretrained_embeddings[0] = 0\n",
    "    try:\n",
    "        word2vec\n",
    "    except:\n",
    "        print('Load word embeddings...')\n",
    "        word2vec = load_bin_vec(os.path.join(bdata,'GoogleNews-vectors-negative300.bin'), word_to_idx)\n",
    "    for word, vector in word2vec.items():\n",
    "        pretrained_embeddings[word_to_idx[word]-1] = vector\n",
    "    # text_field.vocab.load_vectors(wv_type='', wv_dim=300)\n",
    "\n",
    "    model.embeddings.weight.data.copy_(torch.from_numpy(pretrained_embeddings));\n",
    "else:\n",
    "    print('Unknown embedding type {}'.format(EMBEDDING_TYPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual task (B1): Tensorboard instrumentation\n",
    "\n",
    "To get you to work with the some of the basic tools that enable development and tuning of deep learning architectures, we would like you to use Tensorboard.\n",
    "\n",
    "1. read up on how to instrument your code for profiling and visualization in [tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard), e.g. [at this blog](http://www.erogol.com/use-tensorboard-pytorch/)\n",
    "1. [partly done] use the tensorboard `SummaryWriter` to keep track of training loss for each epoch, writing to a local `runs` folder (which is the default)\n",
    "1. launch tensorboard and inspect the log folder, i.e. run `tensorboard --logdir runs` from the assignment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "writer = SummaryWriter(comment='-{}lstm-em{}{}-hid{}-do{}-bs{}-lr{}'\n",
    "                                .format('BI' if USE_BILSTM else '',\n",
    "                                        EMBEDDING_TYPE, EMBEDDING_DIM,\n",
    "                                        HIDDEN_DIM,\n",
    "                                        DROPOUT, BATCH_SIZE, LEARNING_RATE))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da61d157c0a46d0b2a8d46ca02e1c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train epoch 1', max=1384), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: loss 1.53 acc 65.5\n",
      "Dev: loss 6.75 acc 60.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 7.50 acc 57.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe2518a21b945ee99727becbd775e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train epoch 2', max=1384), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: loss 1.27 acc 87.7\n",
      "Dev: loss 6.85 acc 62.6\n",
      "Test: loss 7.57 acc 58.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddf6503a0084880bb046e98342c341b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train epoch 3', max=1384), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: loss 1.14 acc 93.8\n",
      "Dev: loss 6.70 acc 75.7\n",
      "Test: loss 7.40 acc 76.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6425d912f2243a3bf6f86c133fba605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train epoch 4', max=1384), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: loss 1.07 acc 97.0\n",
      "Dev: loss 7.08 acc 73.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e196d1dcf99243099521edb64e4184a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train epoch 5', max=1384), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: loss 1.05 acc 97.5\n",
      "Dev: loss 8.16 acc 60.6\n",
      "Final Test: loss 8.99 acc 56.8\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "trial = 0 # increment this if you manually decide to add more epochs to the current training\n",
    "for epoch in range(EPOCHS*trial,EPOCHS*(trial+1)):\n",
    "    avg_loss, acc = train_epoch_progress(model, train_iter, loss_function, optimizer, text_field, label_field, epoch)\n",
    "    tqdm.write('Train: loss %.2f acc %.1f' % (avg_loss, acc*100))\n",
    "    # TODO: add scalars for training loss and training accuracy to the summary writer\n",
    "    # call the scalars 'Train/Loss' and 'Train/Acc', respectively, and associate them with the current epoch\n",
    "    #...\n",
    "\n",
    "    dev_loss, dev_acc = evaluate(model, dev_iter, loss_function, 'Dev')\n",
    "    # TODO: add scalars for test loss and training accuracy to the summary writer\n",
    "    # call the scalars 'Val/Loss' and 'Val/Acc', respectively, and associate them with the current epoch\n",
    "    #...\n",
    "    \n",
    "    if dev_acc > best_dev_acc:\n",
    "        if best_dev_acc > 0:\n",
    "            os.system('rm '+ out_dir + '/best_model' + '.pth')\n",
    "        best_dev_acc = dev_acc\n",
    "        best_model = model\n",
    "        torch.save(best_model.state_dict(), out_dir + '/best_model' + '.pth')\n",
    "        # evaluate on test with the best dev performance model\n",
    "        test_acc = evaluate(best_model, test_iter, loss_function, 'Test')\n",
    "\n",
    "test_loss, test_acc = evaluate(best_model, test_iter, loss_function, 'Final Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B2: Tune the model\n",
    "\n",
    "After connecting the output of your model train and test performance with tensorboard. Change the model and training parameters above to improve the model performance. We would like to see variable plots of how validation accuracy evolves over a number of epochs for different parameter choices, you can stop exploring when you exceed a model accuracy of 76%.\n",
    "\n",
    "Show a tensorboard screenshot with performance plots that combine at leat 5 different tuning attempts. Store the screenshot as `tensorboard.png`. Then keep the best performing parameters set in this notebook for submission and evaluate the comparison below with your best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison against Vader\n",
    "Vader is a rule-based sentiment analysis algorithm that performs quite well against more complex architectures. The test below is to see, whether LSTMs are able to beat its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vader acc: 0.6880834706205381\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "da = test_iter.data()\n",
    "dat = [(d.text, d.label, ' '.join(d.text)) for d in da]\n",
    "lab_vpred = np.zeros((len(dat), 2))\n",
    "for k, (_, label, sentence) in enumerate(dat):\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    lab_vpred[k,:] = (int(ss['compound']>0), int(label))\n",
    "print('vader acc: {}'.format(1-abs(lab_vpred[:,0]-lab_vpred[:,1]).mean()))\n",
    "writer.add_scalar('Final/VaderAcc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    }
   ],
   "source": [
    "#test_iter.init_epoch\n",
    "batch = list(test_iter)[0]\n",
    "batch.text\n",
    "best_model.eval()\n",
    "pred = best_model(batch.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Bi-)LSTM acc: 0.5551894563426689\n"
     ]
    }
   ],
   "source": [
    "labels = batch.label.data.cpu().detach() - 1\n",
    "labelsnp = labels.cpu().detach().numpy()\n",
    "prednp = pred.data.max(1)[1].cpu().numpy()\n",
    "lstm_acc = 1 - abs(prednp-labelsnp).mean()\n",
    "print('(Bi-)LSTM acc: {}'.format(lstm_acc))\n",
    "writer.add_scalar('Final/LSTMAcc', lstm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the model tuning and training in the previous task until you outperform the Vader algorithm by at least 7% in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save [this notebook](A9.ipynb) containing all cell output and upload your submission as one `A9.ipynb` file.\n",
    "Also, include the screenshot of your tensorboard debugging session as `tensorboard.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some detail notes about the dlenv environment (not needed for this assignment, but maybe for your project)\n",
    "\n",
    "Tensorflow is available in its GPU version now (v1.4.1 based on CUDA 8.0) - before, it was installed in this environment to only run on the CPU.\n",
    "\n",
    "Also, PyTorch is compiled from github using CUDA 9.1, giving the current version 0.4. This enables a feature that was broken for the past two releases - adding the computational graph of any convolutional network model to a tensorboard visualization, e.g. see `demo_graph.py` and other demos in [this repo](https://github.com/lanpa/tensorboard-pytorch), if you'd like to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
